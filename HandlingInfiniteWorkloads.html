<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c0{background-color:#ffffff;padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c7{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c3{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c6{color:inherit;text-decoration:inherit}.c4{height:11pt}.c5{background-color:#ffffff}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c3 doc-content"><p class="c2"><span class="c1 c5">Infinite work streams are the new reality of many systems. Web servers and application servers serve very large user populations where it is realistic to expect infinite streams of new work. The work never ends. Requests come in 24 hours a day 7 days a week. Work could easily saturate servers at 100% CPU usage.</span></p><p class="c0"><span class="c1">Traditionally we have considered 100% CPU usage a bad sign. As compensation we create complicated infrastructures to load balance work, replicate state, and cluster machines.</span></p><p class="c0"><span class="c1">CPUs don&#39;t get tired so you might think we would try to use the CPU as much as possible.</span></p><p class="c0"><span class="c1">In other fields we try to increase productivity by using a resource to the greatest extent possible.</span></p><p class="c0"><span class="c1">In the server world we try to guarantee a certain level of responsiveness by forcing an artificially low CPU usage. The idea is if we don&#39;t have CPU availability then we can&#39;t respond to new work with a reasonable latency or complete existing work.</span></p><p class="c0"><span class="c1">Is there really a problem with the CPU being used 100% of the time? Isn&#39;t the real problem that we use CPU availability and task priority as a simple cognitive shorthand for architecting a system rather than having to understand our system&#39;s low level work streams and using that information to make specific scheduling decisions?</span></p><p class="c0"><span class="c1">We simply don&#39;t have the tools to do anything other than make clumbsy architecture decisions based on load balancing servers and making guesses at the number of threads to use and the priorities for those threads. We could use 100% of CPU time if we could:</span></p><p class="c2"><span class="c1 c5">0. Schedule work so that explicit locking is uncessary (though possible). This</span></p><p class="c2"><span class="c1 c5">&nbsp; &nbsp;will help prevent dead lock and priority inversion.</span></p><p class="c2"><span class="c1 c5">1. Control how much of the CPU work items can have.</span></p><p class="c2"><span class="c1 c5">2. Decide on the relative priority of work and schedule work by </span></p><p class="c2"><span class="c1 c5">&nbsp; &nbsp;that priority.</span></p><p class="c2"><span class="c1 c5">3. Have a fairness algorithm for giving a particular level of service</span></p><p class="c2"><span class="c1 c5">&nbsp; &nbsp;to each work priority.</span></p><p class="c2"><span class="c1 c5">4. Schedule work CPU allowance across tasks.</span></p><p class="c2"><span class="c1 c5">5. Map work to tasks so as to prevent dead lock, priority inversion, and guarantee</span></p><p class="c2"><span class="c1 c5">&nbsp; &nbsp;scheduling latency.</span></p><p class="c2"><span class="c1 c5">6. Have mechanisms to make work give up the CPU after its CPU budget has been used </span></p><p class="c2"><span class="c1 c5">&nbsp; &nbsp;or higher priority works comes in, in such a way to give up locks to prevent </span></p><p class="c2"><span class="c1 c5">&nbsp; &nbsp;dead lock and priority inversion.</span></p><p class="c2"><span class="c1 c5">7. Control new work admission so that back pressure can be put on callers.</span></p><p class="c2"><span class="c1 c5">8. Assing work to objects and process work in order to guarantee protocol ordering.</span></p><p class="c2"><span class="c1 c5">9. Ideally, control work characteristics across the OS and all applications.</span></p><p class="c2 c4"><span class="c1 c5"></span></p><p class="c0"><span class="c1">The problem is we don&#39;t have this level of scheduling control. If we did then a CPU can run at 100% because we have complete control of what work runs when and in what order. There&#39;s no need not to run the CPU at 100% because we know the things we want to run are running.</span></p><p class="c0"><span class="c1">It is interesting frameworks like Struts and JSP rarely even talk about threading. I don&#39;t think application developers really consider the threading architectures of their applications. Popular thread local transaction management mechanisms, for example, assume a request will be satisfied in a single thread. That&#39;s not true of architectures for handling large work loads.</span></p><p class="c0"><span class="c1">Scaling a system requires careful attention to architecture. In the current frameworks applications have very little say as to how their applications run.</span></p><p class="c0"><span>For a discussion of how to handle infinite workloads take a look at </span><span class="c7"><a class="c6" href="https://www.google.com/url?q=https://web.archive.org/web/20071020182702/http://www.possibility.com/epowiki/Wiki.jsp?page%3DArchitectureDiscussion&amp;sa=D&amp;source=editors&amp;ust=1759177800436528&amp;usg=AOvVaw1OHIWE-90P6EOx168yV6ci">Architecture Discussion</a></span><span>&nbsp;and specifically </span><span class="c7"><a class="c6" href="https://www.google.com/url?q=https://web.archive.org/web/20071020182702/http://www.possibility.com/epowiki/Wiki.jsp?page%3DAppBackplane&amp;sa=D&amp;source=editors&amp;ust=1759177800436798&amp;usg=AOvVaw10eQ4MFPdejZPIXD4axPnZ">App Backplane</a></span><span class="c1">.</span></p><p class="c0"><span class="c1">Another way of handling higher work loads is to simply add more machines and have an architecture that supports that type of scaling. While that works, I am addressing the scenarios where you are limitted in your resources, let&#39;s say in embedded system or in scenario where you can only afford one or two machine in a colo site.</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c4"><span class="c1"></span></p></body></html>